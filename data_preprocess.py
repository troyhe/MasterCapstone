# -*- coding: utf-8 -*-
"""Data-Preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10MR6RbS8GvBtS1i05sJ42J7P0uXgM4pZ
"""

import sys
import os
import re
import codecs
import torch
import json
import numpy
import datetime

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/gdrive')
# %cd gdrive/MyDrive/Twibot-20

def count_matches(text, pattern):
    return len(re.findall(pattern, text))

def mkdir(path):
    if not os.path.isdir(path):
        os.mkdir(path)

def check_repeat(word):
	chars = "abcdefghijklmnopqrstuvwxyz"
	count = {}
	for s in word:
		if s in count:
			count[s] += 1
		else:
			count[s] = 1
	for key in count:
		if count[key] > 1:
			return True
	return False

def word_preprocess(word, tweet):
	if re.match(r'^https?:/{2}\w.+$', word):
		tweet.append('<url>')
	elif word[0] == '#':
		tweet.append('<hashtag>')
	elif word[0] == '@':
		tweet.append('<user>')
	elif word.isdigit():
		tweet.append('<number>')
	elif (word.upper() == word and check_repeat(word.lower())):
		tweet.append(word.lower())
		tweet.append('<allcaps>')
	else: 
		tweet.append(word.lower())

from gensim.parsing.preprocessing import preprocess_string,  strip_tags, strip_multiple_whitespaces

RE_PUNCT = re.compile(r'([%s])+' % re.escape("""$%&'()*+, -./:;=?[\]"^<_>`{|}~"""), re.UNICODE)

def strip_punctuation(s):
    return RE_PUNCT.sub(" ", s)

def prepare_dataset(file_list, out):
  for file in file_list:
    human = []
    bot = []
    print(file)
    f = open(file + '.json')
    users = json.load(f)
    count = 0
    for user in users:
        count += 1
        if (count == 400 and file == 'test'):
          break
        if (count == 3000):
          break
        tweets_text = []
        stats = {'RT': 0, 'emoji': 0, 'URL': 0, 'mention': 0}
        label = user["label"]
        tweets = user["tweet"]
        if (tweets == None):
           continue
        num_tweets = 0
        for tweet in tweets:
          num_tweets += 1;
          t_data = tweet
          if t_data[0:2] == 'RT':
              stats['RT'] += 1
              continue

          stats['URL'] += count_matches(t_data, 'https://t\\.co')
          stats['mention'] += count_matches(t_data, '@')
          stats['emoji'] += count_matches(t_data, '(\\u00a9|\\u00ae|[\\u2000-\\u3300]|\\ud83c['
                                                  '\\ud000-\\udfff]|\\ud83d[\\ud000-\\udfff]|\\ud83e['
                                                  '\\ud000-\\udfff])')
          
          text = []
          for word in tweet.split():
            word_preprocess(word, text)
          
          # tweet_text = preprocess_string(' '.join(text), filters=[strip_tags, strip_punctuation, strip_multiple_whitespaces])
          # if (count <= 10 and num_tweets <= 10):
          #   print(' '.join(text))
          #   print(' '.join(tweet_text))

          tweets_text.append(' '.join(text))
          
          if (num_tweets >= 100):
            break
        tweets_package = f"{stats}:::" + ' '.join(tweets_text)
        if (label == '0'): 
          human.append(tweets_package)
        else:
          bot.append(tweets_package)
        
    print(count)
    print("Bot " + file +  "-set size: ", len(bot))
    print("Human " + file +  "-set size: ", len(human))
    
    out_dir = os.path.join(out, file) 
    mkdir(out_dir)

    with codecs.open(os.path.join(out_dir, 'human_male.txt'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(human))
    with codecs.open(os.path.join(out_dir, 'bot.txt'), 'w', encoding='utf-8') as f:
            f.write('\n'.join(bot))



out = os.path.normpath('dataset-light-3000')
mkdir(out)
file_list = ['train', 'test']
prepare_dataset(file_list, out)